# -*- coding: utf-8 -*-
"""RegressãoNãoLinear.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u1DTgGWdjpppn0omUjrpZwNaxL-siasF
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import make_moons
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor,AdaBoostClassifier, AdaBoostRegressor, GradientBoostingClassifier, GradientBoostingRegressor
from sklearn import metrics

from sklearn.tree import export_graphviz
import pydot

df = pd.read_excel("house.xlsx")

df.head()

df.describe()

df.info()

# One-hot encode the data using pandas get_dummies
df = pd.get_dummies(df)

df.dropna(inplace=True)

df.head()

y = np.array(df["SalePrice"])

x = df.drop("SalePrice", axis=1)

x_list = list(df.columns)

casa = np.array(x)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state = 42)

"""# Treino com RandomForestRegressor"""

# Instantiate model with 1000 decision trees
rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)
# Train the model on training data
rf.fit(x_train, y_train);

# Use the forest's predict method on the test data
predictions_rf = rf.predict(x_test)

# Calculate the absolute errors
errors_rf = abs(predictions_rf - y_test)

# Print out the mean absolute error (mae)
print('Mean Absolute Error:', round(np.mean(errors_rf), 2), 'degrees.')

r_sq = rf.score(x, y)
print('Coeficiente de Determinação (R²):', r_sq)

print('MAE:', metrics.mean_absolute_error(y_test, predictions_rf))
print('MSE:', metrics.mean_squared_error(y_test, predictions_rf))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions_rf)))

"""# Treino com AdaBoostRegressor"""

ada = AdaBoostRegressor(n_estimators=100)
ada.fit(x_train, y_train)
ada_pred = ada.predict(x_test)

# Calculate the absolute errors
errors_ada = abs(ada_pred - y_test)

# Print out the mean absolute error (mae)
print('Mean Absolute Error:', round(np.mean(errors_ada), 2), 'degrees.')

r_sq = ada.score(x, y)
print('Coeficiente de Determinação (R²):', r_sq)

print('MAE:', metrics.mean_absolute_error(y_test, ada_pred))
print('MSE:', metrics.mean_squared_error(y_test, ada_pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, ada_pred)))

"""# Treino com GradientBoostingRegressor"""

grb = GradientBoostingRegressor(n_estimators=100)
grb.fit(x_train, y_train)
gbr_pred = grb.predict(x_test)

# Calculate the absolute errors
errors_gbr = abs(gbr_pred - y_test)

# Print out the mean absolute error (mae)
print('Mean Absolute Error:', round(np.mean(errors_gbr), 2), 'degrees.')

r_sq = grb.score(x, y)
print('Coeficiente de Determinação (R²):', r_sq)

print('MAE:', metrics.mean_absolute_error(y_test, gbr_pred))
print('MSE:', metrics.mean_squared_error(y_test, gbr_pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, gbr_pred)))

"""# Visualização de árvores - Random Forest"""

# Assuming 'Id' is the extra column, drop it from the feature names
feature_names = df.drop(columns=['SalePrice']).columns

rf = RandomForestRegressor(max_depth = 3)
rf.fit(x_train, y_train)

# Pull out one tree from the forest
tree = rf.estimators_[5]

# Export the image to a dot file
export_graphviz(tree, out_file = 'tree.dot', feature_names = feature_names, rounded = True, precision = 1)

# Use dot file to create a graph
(graph, ) = pydot.graph_from_dot_file('tree.dot')

# Write graph to a png file
graph.write_png('tree.png')

